# STAT5243-project3
project3 - A/B test


### ğŸ“ `STAT5243_project_2`
**Description:**  
è¿™æ˜¯ä¸€ä¸ªåŸºäº [Python Shiny](https://shiny.posit.co/py/) å®ç°çš„äº¤äº’å¼æ•°æ®æ¸…æ´—ä¸ç‰¹å¾å·¥ç¨‹å¹³å°ï¼Œä¸»è¦ç”¨äºæ‰§è¡Œ A/B æµ‹è¯•å®éªŒã€‚å®éªŒæ ¸å¿ƒç›®æ ‡æ˜¯æµ‹è¯•æç¤ºæ–‡æœ¬é¢œè‰²ï¼ˆå¦‚çº¢è‰² vs é»‘è‰²ï¼‰å¯¹ç”¨æˆ·è¡Œä¸ºçš„å½±å“ã€‚

**ğŸ”¬ A/B Testing Objective:**  
æˆ‘ä»¬å°† Note çš„æç¤ºé¢œè‰²ï¼ˆä¾‹å¦‚ `çº¢è‰²` vs `é»‘è‰²`ï¼‰ä½œä¸ºå®éªŒå˜é‡ï¼Œéšæœºåˆ†é…ç”¨æˆ·è‡³ A/B ä¸¤ç»„ï¼š

- **Group A:** æ˜¾ç¤ºçº¢è‰²æç¤ºæ–‡æœ¬ï¼ˆé»˜è®¤æ ·å¼ï¼‰
- **Group B:** æ˜¾ç¤ºé»‘è‰²æç¤ºæ–‡æœ¬ï¼ˆå»å¼ºè°ƒï¼‰

**ğŸ¯ æµ‹é‡æŒ‡æ ‡ï¼š**

æ¯ä¸ªç”¨æˆ·ä¼šè¯é€šè¿‡æ—¥å¿—ç³»ç»Ÿè®°å½•ä»¥ä¸‹æ“ä½œè¡Œä¸ºæŒ‡æ ‡ï¼š

- `apply_fe_button_clicked_count`: åº”ç”¨äº†ç‰¹å¾å·¥ç¨‹çš„ç‚¹å‡»æ¬¡æ•°
- `apply_fe_button_error_count`: ç‚¹å‡»æ—¶å‡ºé”™æ¬¡æ•°
- `revert_button_clicked_count`: æ’¤é”€ä¿®æ”¹ç‚¹å‡»æ¬¡æ•°
- `download_button_clicked_count`: ä¸‹è½½æ•°æ®çš„ç‚¹å‡»æ¬¡æ•°
- `download_button_clicked_time`: ä¸‹è½½æŒ‰é’®é¦–æ¬¡ç‚¹å‡»æ—¶é—´
- `*_clicked_rate`: æ¯ä¸ªæŒ‰é’®ç‚¹å‡»æ¬¡æ•° / æ€»ä¼šè¯æ—¶é—´
- `session_start_time`, `session_end_time`, `total_session_time`

æ•°æ®ä¼šè¢«å†™å…¥æœ¬åœ° CSV æ–‡ä»¶ `session_log.csv`ï¼Œä¹Ÿå¯ä»¥é€šè¿‡ HTTP POST å‘é€è‡³æ—¥å¿—æœåŠ¡å™¨ã€‚

**ğŸ§© åŠŸèƒ½æ¨¡å—ï¼š**

- æ•°æ®æ–‡ä»¶ä¸Šä¼ ä¸å†…ç½®æ•°æ®åŠ è½½ï¼ˆæ”¯æŒ CSV, Excel, JSON, RDS ç­‰ï¼‰
- å˜é‡é€‰æ‹©ä¸æ•°æ®æ¸…æ´—
- ç¼ºå¤±å€¼å¤„ç†ï¼ˆè½¬ä¸º NAã€å‡å€¼/ä¼—æ•°å¡«å……ã€åˆ—è¡¨åˆ é™¤ï¼‰
- ç‰¹å¾å·¥ç¨‹æ“ä½œï¼ˆå½’ä¸€åŒ–ã€ç‹¬çƒ­ç¼–ç ã€æ—¥æœŸè½¬å¤©æ•°ã€Box-Cox è½¬æ¢ï¼‰
- å¯è§†åŒ–ï¼ˆçº¿å›¾ã€æŸ±å›¾ã€æ•£ç‚¹å›¾ã€ç›´æ–¹å›¾ã€ç›¸å…³çƒ­åŠ›å›¾ï¼‰
- æ—¥å¿—ç³»ç»Ÿè‡ªåŠ¨è®°å½•ç”¨æˆ·è¡Œä¸ºæ•°æ®ç”¨äºå®éªŒè¯„ä¼°


## ğŸ“Š STAT5243 Log Server

è¿™æ˜¯ä¸€ä¸ªåŸºäº **Flask** çš„è½»é‡çº§æ—¥å¿—æœåŠ¡å™¨ï¼Œéƒ¨ç½²åœ¨ **Render** ä¸Šï¼Œç”¨äºæ¥æ”¶å’Œè®°å½•æ¥è‡ªå‰ç«¯ Shiny Web App çš„ç”¨æˆ·è¡Œä¸ºæ•°æ®ï¼ˆå¦‚ç‚¹å‡»æ¬¡æ•°ã€ä¼šè¯æ—¶é•¿ç­‰ï¼‰ï¼Œæ”¯æŒ A/B æµ‹è¯•å®éªŒåˆ†æã€‚

---

## ğŸŒ é¡¹ç›®éƒ¨ç½²åœ°å€

- **æœåŠ¡ä¸»é¡µ**ï¼š  
  [`https://stat5243-project3-1.onrender.com`](https://stat5243-project3-1.onrender.com)

- **æ—¥å¿—ä¸Šä¼ æ¥å£**ï¼š  
  `POST https://stat5243-project3-1.onrender.com/log`

- **çŠ¶æ€æŸ¥çœ‹æ¥å£**ï¼š  
  `GET  https://stat5243-project3-1.onrender.com/status`

---

## ğŸ“¦ é¡¹ç›®ç»“æ„

```
STAT5243_log_server/
â”œâ”€â”€ server.py              # Flask ä¸»ç¨‹åº
â”œâ”€â”€ requirements.txt       # ä¾èµ–é¡¹åˆ—è¡¨
â””â”€â”€ logs/                  # æ¯æ—¥ç”Ÿæˆçš„ CSV æ—¥å¿—ï¼ˆè‡ªåŠ¨åˆ›å»ºï¼‰
```

---

## ğŸ”§ Render é…ç½®è¯´æ˜

| é…ç½®é¡¹         | è®¾ç½®å€¼                         |
|----------------|--------------------------------|
| ç±»å‹           | Web Service                    |
| Build Command  | `pip install -r requirements.txt` |
| Start Command  | `python server.py`             |
| Port           | è‡ªåŠ¨ç»‘å®šåˆ° Flask çš„ 5000 ç«¯å£  |

> âš ï¸ ä¸éœ€è¦æ‰‹åŠ¨é…ç½®ç«¯å£ï¼ŒRender ä¼šè‡ªåŠ¨æ£€æµ‹ 0.0.0.0:5000ã€‚


## ğŸ“ æ•°æ®å­˜å‚¨å½¢å¼ï¼ˆæ¯æ—¥ä¸€ä¸ª CSVï¼‰

æ—¥å¿—å°†è‡ªåŠ¨ä»¥æ¯æ—¥æ—¥æœŸå‘½åä¿å­˜åœ¨ `logs/` æ–‡ä»¶å¤¹ä¸­ï¼š

```bash
logs/session_log_20250418.csv
```

å­—æ®µç¤ºä¾‹ï¼š

| user_id | group | session_start_time | ... | download_button_clicked_count |
|---------|-------|--------------------|-----|-------------------------------|
| abcd123 | B     | 2024-04-18T10:00   | ... | 2                             |



---

--------------------------------------------------------

Statistical Analysis - Nan Xiao

Summarization of the steps:
We've created a Python script for analyzing an AB test dataset comparing "red" and "gray" groups.
The script performs the following analysis for each metric:
Computes descriptive statistics for each group
Tests for normality to decide which statistical test to use
Performs either t-tests (for normal data) or Mann-Whitney U tests (for non-normal data)
Creates bar plots comparing the groups with significance indicators
Adds detailed statistics and test results on the plots
The script saves all plots to a dedicated directory (C:/Users/å‡¡æ›²/AB_Test_Results) and also attempts to display them in windows.
A summary CSV file with all statistical test results is also saved.

Summary of Analysis Results
Based on the analysis of the AB test data comparing "red" and "gray" groups, here are the key findings:
Clicked Rate: No statistically significant difference between red and gray groups (p=0.3584)
Clicked Count: Statistically significant difference (p=0.0211)
Gray group has higher clicked count (7.94 vs 6.81)
Gray interface performs 15.4% better in this metric
Session Time: No statistically significant difference (p=0.6929)
Error Count: Statistically significant difference (p<0.0001)
Red group has lower error count (0.47 vs 1.10)
Users are 80.3% less likely to make errors with the red interface
Error Rate: Statistically significant difference (p=0.0001)
Red group has lower error rate (0.0037 vs 0.0065)
Users show 54.9% lower error rate with the red interface
Overall Conclusion
The red interface appears to perform better overall, winning in 2 out of 3 significant metrics (error count and error rate). While the gray interface leads in clicked count, the red interface demonstrates superior performance in reducing errors.
The analysis shows that:
Users click more with the gray interface
Users make fewer errors with the red interface
Session time isn't significantly different between interfaces
Recommendation
Based on the analysis, the red interface would be the recommended choice if minimizing errors is the primary goal. However, if higher engagement (more clicks) is prioritized, the gray interface might be preferred.
All the plots, detailed statistics, and conclusions have been saved to C:/Users/å‡¡æ›²/AB_Test_Results/ for further reference.


Data claening -Qiaoyang Lin
# 1. total_session_time in seconds
# 2. total_clicked_count
# 3. total_error_count
# 4. Clicked and error rates
# 5. Count number of each type of error in the operation column

Detecting Outliers with 3.0 IQR, but we found that there's a lot of data to be eliminated. 
We conclude that if variables like apply_fe_button_clicked_count or total_session_time are highly right-skewed (mostly low values, a few oversized), 
even with 3 Ã— IQR, all samples with long right tails will still be recognized as exceptions.
Thus, we use other methods to cross-validate. Method 2: Detecting Outliers with Z-score and Method 3: Detecting Outliers with DBSCAN.
Finally, we identify users who are detected as abnormal by all three methods


